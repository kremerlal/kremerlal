{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fannie Mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final class project to predict the net loss on a mortgage loan, using public Fannie Mae data from 1991 - 2021. Started code provided by the professor, a group effort to finalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, LabelEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_predict\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from patsy import dmatrices, dmatrix, build_design_matrices\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set number of CPU cores for parallel algorithms\n",
    "import os\n",
    "if \"CPU_LIMIT\" in os.environ:\n",
    "    # If you are on JupyterHub, this gives you the right number of CPUs for your virtual machine\n",
    "    num_cpus = int(os.getenv(\"CPU_LIMIT\").split('.')[0])\n",
    "else:\n",
    "    # If you are not on JupyterHub, this gives you the right number for your computer.\n",
    "    num_cpus = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes it so that the pandas dataframes don't get truncated horizontally.\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_classes = {\"LOAN_IDENTIFIER\": np.character, \n",
    "               \"CHANNEL\": 'category', \n",
    "               \"SELLER_NAME\": np.character, \n",
    "               \"ORIGINAL_INTEREST_RATE\": np.float32, \n",
    "               \"ORIGINAL_UPB\": np.float64,\n",
    "               \"ORIGINAL_LOAN_TERM\": \"Int16\", \n",
    "               \"ORIGINATION_DATE\": np.character,\n",
    "               \"FIRST_PAYMENT_DATE\": np.character, \n",
    "               \"ORIGINAL_LTV\": np.float32, \n",
    "               \"ORIGINAL_COMBINED_LTV\": np.float32, \n",
    "               \"NUMBER_OF_BORROWERS\": 'category', \n",
    "               \"DTI\": np.float32, \n",
    "               \"BORROWER_CREDIT_SCORE_AT_ORIGINATION\": \"UInt16\", \n",
    "               \"COBORROWER_CREDIT_SCORE_AT_ORIGINATION\": 'UInt16', \n",
    "               \"FIRST_TIME_HOME_BUYER_INDICATOR\": 'category', \n",
    "               \"LOAN_PURPOSE\": 'category', \n",
    "               \"PROPERTY_TYPE\": 'category',\n",
    "               \"NUMBER_OF_UNITS\": \"UInt16\", \n",
    "               \"OCCUPANCY_STATUS\": 'category', \n",
    "               \"PROPERTY_STATE\": 'category', \n",
    "               \"MSA\": 'category', \n",
    "               \"ZIP_CODE_SHORT\": 'category', \n",
    "               \"MORTGAGE_INSURANCE_PERCENTAGE\": np.float32, \n",
    "               \"AMORTIZATION_TYPE\": np.character,\n",
    "               \"MORTGAGE_INSURANCE_TYPE\": 'category', \n",
    "               \"RELOCATION_MORTGAGE_INDICATOR\": 'category',\n",
    "               \"CREDIT_SCORE_MIN\": \"UInt16\",\n",
    "               \"ORIGINAL_VALUE\": float,\n",
    "               \"ZERO_BALANCE_CODE\": 'category',\n",
    "               \"LOAN_AGE\": \"Int16\",\n",
    "               \"NET_LOSS\": float,\n",
    "               \"NET_SEVERITY\": float,\n",
    "               \"LAST_STAT\": 'category',\n",
    "               \"LOAN_MODIFICATION_COSTS\": float,\n",
    "               \"TOTAL_LOSSES\": float,\n",
    "               \"MSA_NAME\": 'category',\n",
    "               \"CENSUS_2010_POP\": float}\n",
    "\n",
    "date_columns = [\"ORIGINATION_DATE\",\n",
    "                \"FIRST_PAYMENT_DATE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "full_data_set = False\n",
    "\n",
    "FILES_LOCATION = '/DataAnalyticsI/'\n",
    "\n",
    "if not full_data_set:\n",
    "    df_train = pd.read_csv(FILES_LOCATION + \"FannieMaeSmallTrain.csv\",\n",
    "                           index_col=\"LOAN_IDENTIFIER\",\n",
    "                           dtype=col_classes,\n",
    "                           parse_dates=date_columns,\n",
    "                           sep='|')\n",
    "    df_test = pd.read_csv(FILES_LOCATION + \"FannieMaeSmallTest.csv\",\n",
    "                          index_col=\"LOAN_IDENTIFIER\",\n",
    "                          dtype=col_classes,\n",
    "                          parse_dates=date_columns,\n",
    "                          sep='|')\n",
    "\n",
    "if full_data_set:\n",
    "    # This p is the proportion of the training data you load.\n",
    "    # You can set it anywhere from 0 to 1.\n",
    "    p = 1\n",
    "    random.seed(201)\n",
    "    df_train = pd.read_csv(FILES_LOCATION + \"FannieMaeTrain.csv\",\n",
    "                           index_col=\"LOAN_IDENTIFIER\",\n",
    "                           dtype=col_classes,\n",
    "                           parse_dates=date_columns,\n",
    "                           sep='|',\n",
    "                           skiprows=lambda i: i>0 and random.random() > p)\n",
    "    df_test = pd.read_csv(FILES_LOCATION + \"FannieMaeTest.csv\",\n",
    "                          index_col=\"LOAN_IDENTIFIER\",\n",
    "                          dtype=col_classes,\n",
    "                          parse_dates=date_columns,\n",
    "                          sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ZERO_BALANCE_CODE' in df_train:\n",
    "    df_train.drop(['ZERO_BALANCE_CODE', 'LOAN_AGE', 'NET_SEVERITY', 'LAST_STAT', 'LOAN_MODIFICATION_COSTS', 'TOTAL_LOSSES'],\n",
    "                  axis=1,\n",
    "                  inplace=True)\n",
    "if 'ZERO_BALANCE_CODE' in df_test:\n",
    "    df_test.drop(['ZERO_BALANCE_CODE', 'LOAN_AGE', 'NET_SEVERITY', 'LAST_STAT', 'LOAN_MODIFICATION_COSTS', 'TOTAL_LOSSES'],\n",
    "                  axis=1,\n",
    "                  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataframe(df):\n",
    "    \"\"\"Summarize a dataframe, and report missing values.\"\"\"\n",
    "    missing_values = pd.DataFrame({'Variable Name': df.columns,\n",
    "                                   'Data Type': df.dtypes,\n",
    "                                   'Missing Values': df.isnull().sum(),\n",
    "                                   'Unique Values': [df[name].nunique() for name in df.columns]}\n",
    "                                 ).set_index('Variable Name')\n",
    "    with pd.option_context(\"display.max_rows\", 1000):\n",
    "        display(pd.concat([missing_values, df.describe(include='all', datetime_is_numeric=True).transpose()], axis=1).fillna(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_dataframe(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_dataframe(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineer Row Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['ORIGINATION_DATE'] = pd.to_datetime(df_train['ORIGINATION_DATE'], format='%Y-%m-%d')\n",
    "\n",
    "df_test['ORIGINATION_DATE'] = pd.to_datetime(df_test['ORIGINATION_DATE'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['YEAR'] = df_train['ORIGINATION_DATE'].dt.year\n",
    "df_test['YEAR'] = df_test['ORIGINATION_DATE'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = \"2005/07/01\"\n",
    "date2 = \"2008/02/01\"\n",
    "\n",
    "newdate1 = pd.to_datetime(date1, format='%Y/%m/%d')\n",
    "newdate2 = pd.to_datetime(date2, format='%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['BadYears'] = df_train['ORIGINATION_DATE'].apply(lambda x: True if (x>newdate1 and x<newdate2) else False)\n",
    "df_test['BadYears'] = df_test['ORIGINATION_DATE'].apply(lambda x: True if (x>newdate1 and x<newdate2) else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['BadState'] = df_train['PROPERTY_STATE'].apply(lambda x: True if (x=='NV' or x=='FL' or x=='AZ' or x=='CA' or x=='MI') else False)\n",
    "df_test['BadState'] = df_test['PROPERTY_STATE'].apply(lambda x: True if (x=='NV' or x=='FL' or x=='AZ' or x=='CA' or x=='MI') else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Into Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smaller_train, df_validation = train_test_split(df_train, test_size = 0.25, random_state = 201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smaller_train = df_smaller_train.copy()\n",
    "df_validation = df_validation.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CategoricalImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom defined imputer for categorical data. This allows you to specify an \n",
    "    other class where any category that doesn't meet the requirements necessary to\n",
    "    be in \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, other_threshold=0, \n",
    "                 other_label=\"OTHER\",\n",
    "                 missing_first=True,\n",
    "                 missing_values=np.nan, \n",
    "                 strategy='constant', \n",
    "                 fill_value=\"MISSING\", \n",
    "                 verbose=0, \n",
    "                 copy=True, \n",
    "                 add_indicator=False):\n",
    "        self.add_indicator = add_indicator\n",
    "        self.copy=copy\n",
    "        self.verbose=verbose\n",
    "        self.fill_value=fill_value\n",
    "        self.missing_first=missing_first\n",
    "        self.missing_values=missing_values\n",
    "        self.other_label=other_label\n",
    "        self.other_threshold=other_threshold\n",
    "        self.strategy=strategy\n",
    "        if hasattr(missing_values, \"__iter__\"):\n",
    "            self.missing_values = missing_values\n",
    "        else:\n",
    "            self.missing_values = [missing_values]\n",
    "        self._imputer = SimpleImputer(missing_values=missing_values, strategy=strategy, fill_value=fill_value, verbose=verbose, copy=copy, add_indicator=False)\n",
    "        self._column_categories = {}\n",
    "\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if type(self.other_threshold) == int or type(self.other_threshold) == float:\n",
    "            other_threshold = [self.other_threshold]*len(X.columns)\n",
    "        elif len(self.other_threshold) == len(X.columns):\n",
    "            other_threshold = self.other_threshold\n",
    "        else:\n",
    "            raise TypeError(\"other_threshold must be either a single number or a list of numbers equal to the number of columns.\")\n",
    "\n",
    "        i = 0\n",
    "        X = X.copy()\n",
    "        X = X[:].astype(object)\n",
    "        if self.missing_first:\n",
    "            X = pd.DataFrame(self._imputer.fit_transform(X), columns=X.columns, index=X.index)\n",
    "        column_categories = {}\n",
    "        for column in X.columns:\n",
    "            if other_threshold[i] < 1:\n",
    "                other_threshold[i] = other_threshold[i]*X[column].shape[0]\n",
    "            \n",
    "            value_counts = X[column].value_counts()\n",
    "            categories = [category for category in value_counts.index if value_counts.loc[category] >= other_threshold[i]]\n",
    "            if value_counts.iloc[-1] >= other_threshold[i]:\n",
    "                categories[-1] = self.other_label\n",
    "            else:\n",
    "                categories.append(self.other_label)\n",
    "            \n",
    "            self._column_categories[column] = categories\n",
    "            i = i + 1\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X = X[:].astype(object)\n",
    "        if self.missing_first:\n",
    "            X = pd.DataFrame(self._imputer.fit_transform(X), columns=X.columns, index=X.index)\n",
    "        for column in X.columns:\n",
    "            X.loc[~X[column].isin(self._column_categories[column]) & ~X[column].isin(self.missing_values), column] = self.other_label\n",
    "        return pd.DataFrame(self._imputer.fit_transform(X), columns=X.columns, index=X.index)[:].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_smaller_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer_zero = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "imputer_missing = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')\n",
    "categorical_imputer = CategoricalImputer(other_threshold=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_mean = ['ORIGINAL_INTEREST_RATE',\n",
    "                   'ORIGINAL_UPB',\n",
    "                   'ORIGINAL_LOAN_TERM',\n",
    "                   'ORIGINAL_LTV',\n",
    "                   'ORIGINAL_COMBINED_LTV',\n",
    "                   'ORIGINAL_VALUE',\n",
    "                   'YEAR']\n",
    "\n",
    "continuous_zero = ['MORTGAGE_INSURANCE_PERCENTAGE', \n",
    "                   'BORROWER_CREDIT_SCORE_AT_ORIGINATION',\n",
    "                   'COBORROWER_CREDIT_SCORE_AT_ORIGINATION',\n",
    "                   'CREDIT_SCORE_MIN',\n",
    "                   'DTI']\n",
    "\n",
    "continuous_variables = continuous_mean + continuous_zero\n",
    "\n",
    "categorical_missing = ['FIRST_TIME_HOME_BUYER_INDICATOR',\n",
    "                       'MORTGAGE_INSURANCE_TYPE','BadYears','BadState']\n",
    "\n",
    "categorical_variables = ['CHANNEL',\n",
    "                         'SELLER_NAME',\n",
    "                         'NUMBER_OF_BORROWERS',\n",
    "                         'LOAN_PURPOSE',\n",
    "                         'PROPERTY_TYPE',\n",
    "                         'NUMBER_OF_UNITS',\n",
    "                         'OCCUPANCY_STATUS',\n",
    "                         'PROPERTY_STATE',\n",
    "                         'ZIP_CODE_SHORT',\n",
    "                         'AMORTIZATION_TYPE',\n",
    "                         'RELOCATION_MORTGAGE_INDICATOR',\n",
    "                         'MSA',\n",
    "                         'MSA_NAME',\n",
    "                         'CENSUS_2010_POP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_mean.fit(df_smaller_train[continuous_mean])\n",
    "df_smaller_train[continuous_mean] = imputer_mean.transform(df_smaller_train[continuous_mean])\n",
    "df_validation[continuous_mean] = imputer_mean.transform(df_validation[continuous_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_zero.fit(df_smaller_train[continuous_zero])\n",
    "df_smaller_train[continuous_zero] = imputer_zero.transform(df_smaller_train[continuous_zero])\n",
    "df_validation[continuous_zero] = imputer_zero.transform(df_validation[continuous_zero])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_imputer.fit(df_smaller_train[categorical_variables])\n",
    "df_smaller_train[categorical_variables] = categorical_imputer.transform(df_smaller_train[categorical_variables])\n",
    "df_validation[categorical_variables] = categorical_imputer.transform(df_validation[categorical_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_missing.fit(df_smaller_train[categorical_missing])\n",
    "df_smaller_train[categorical_missing] = imputer_missing.transform(df_smaller_train[categorical_missing])\n",
    "df_validation[categorical_missing] = imputer_missing.transform(df_validation[categorical_missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_dataframe(df_smaller_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_loss = df_train['NET_LOSS'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"Function that returns a table showing RMSE and MAE.\"\"\"\n",
    "    RMSE = mean_squared_error(y_true, y_pred)**(1/2)\n",
    "    naive_RMSE = mean_squared_error(y_true, [average_loss]*len(y_true))**(1/2)\n",
    "    acc_df = pd.DataFrame(data = {\"RMSE\": [RMSE],\n",
    "                                  \"Naive - RMSE\": [naive_RMSE - RMSE]})\n",
    "    display(acc_df.style.hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Engineering for Tree Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features_trees = ['ORIGINAL_LTV', 'DTI', 'CREDIT_SCORE_MIN','ORIGINAL_INTEREST_RATE','ORIGINAL_UPB','COBORROWER_CREDIT_SCORE_AT_ORIGINATION','BORROWER_CREDIT_SCORE_AT_ORIGINATION']\n",
    "cat_ordinal_features_trees = ['PROPERTY_STATE','OCCUPANCY_STATUS','LOAN_PURPOSE','MORTGAGE_INSURANCE_TYPE','NUMBER_OF_BORROWERS','BadState']\n",
    "cat_dummy_features_trees = ['FIRST_TIME_HOME_BUYER_INDICATOR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tree_train = df_smaller_train[continuous_features_trees + cat_ordinal_features_trees]\n",
    "y_tree_train = df_smaller_train['NET_LOSS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_tree = \"0 + \" + \" + \".join(cat_dummy_features_trees)  + \" + YEAR:MORTGAGE_INSURANCE_TYPE\" + \" + BadYears:MSA\" + \" + YEAR:BadState\" + \" + YEAR:ORIGINAL_INTEREST_RATE\" + \" + YEAR:C(CHANNEL)\" + \" + FIRST_TIME_HOME_BUYER_INDICATOR:BORROWER_CREDIT_SCORE_AT_ORIGINATION:ORIGINAL_INTEREST_RATE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tree_train_patsy = dmatrix(formula_tree, df_smaller_train, return_type=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tree_train_patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tree_train = pd.concat([X_tree_train, X_tree_train_patsy], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tree_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "ordinal_encoder.fit(X_tree_train[cat_ordinal_features_trees])\n",
    "X_tree_train[cat_ordinal_features_trees] = ordinal_encoder.transform(X_tree_train[cat_ordinal_features_trees])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tree_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tree_train.columns = X_tree_train.columns.str.replace('[', '(').str.replace(']', ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tree_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and transform our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tree_validation = df_validation[continuous_features_trees + cat_ordinal_features_trees]\n",
    "y_tree_validation = df_validation['NET_LOSS']\n",
    "\n",
    "X_tree_validation_patsy = build_design_matrices([X_tree_train_patsy.design_info], df_validation, return_type=\"dataframe\")[0]\n",
    "\n",
    "X_tree_validation = pd.concat([X_tree_validation, X_tree_validation_patsy], axis=1)\n",
    "\n",
    "X_tree_validation[cat_ordinal_features_trees] = ordinal_encoder.transform(X_tree_validation[cat_ordinal_features_trees])\n",
    "\n",
    "X_tree_validation.columns = X_tree_validation.columns.str.replace('[', '(').str.replace(']', ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search--don't run during final export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [5, 10, 15],\n",
    "              'min_samples_split': [500, 1000, 2500, 7500, 10000, 2500],\n",
    "              'min_impurity_decrease': [0, .001, .01, .1, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "list(ParameterGrid(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs_rt_model = GridSearchCV(DecisionTreeRegressor(max_features = .5, ccp_alpha=10, random_state=201), param_grid=parameters, cv=4, n_jobs=num_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs_rt_model.fit(X_tree_train, y_tree_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs_rt_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start running again here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dt_model = DecisionTreeRegressor(max_depth=10,\n",
    "                                 min_samples_split=1000,\n",
    "                                 max_features=.5,\n",
    "                                 min_impurity_decrease=0,\n",
    "                                 random_state=201)\n",
    "dt_model.fit(X_tree_train, y_tree_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Importance': dt_model.feature_importances_}, index=X_tree_train.columns).sort_values(['Importance'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pred = dt_model.predict(X_tree_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(df_validation['NET_LOSS'], dt_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search--don't run during final export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': [10, 15, 20],\n",
    "              'min_samples_split': [100, 250, 500],\n",
    "              'max_features': [10,  15,  20],\n",
    "              'n_estimators': [75, 100, 200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "list(ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rt_model = GridSearchCV(RandomForestRegressor(max_features = .5, ccp_alpha=10, random_state=201), param_grid=param_grid, cv=2, n_jobs=num_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rt_model.fit(X_tree_train, y_tree_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rt_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start running again here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_model = RandomForestRegressor(n_estimators=200,\n",
    "                                 max_features=20,\n",
    "                                 max_depth=20,\n",
    "                                 min_samples_split=100,\n",
    "                                 min_impurity_decrease=0,\n",
    "                                 random_state=201,\n",
    "                                 n_jobs=num_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_tree_train, y_tree_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Importance': rf_model.feature_importances_}, index=X_tree_train.columns).sort_values(['Importance'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = rf_model.predict(X_tree_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(df_validation['NET_LOSS'], rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosted trees model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search--don't run during final export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parametersxb = {'max_depth': [3, 6, 10, 12],\n",
    "              'n_estimators': [90, 100, 110, 130],\n",
    "            'learning_rate': [.3, .2, .1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "list(ParameterGrid(parametersxb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs_rt_modelxb = GridSearchCV(XGBRegressor(),parametersxb, cv=2, n_jobs=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs_rt_modelxb.fit(X_tree_train, y_tree_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs_rt_modelxb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start running again here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(max_depth=3,\n",
    "                         n_estimators = 90,\n",
    "                         learning_rate=.2,\n",
    "                         ccp_alpha=10,\n",
    "                         random_state=201)\n",
    "xgb_model.fit(X_tree_train, y_tree_train)\n",
    "xgb_pred = xgb_model.predict(X_tree_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Importance': xgb_model.feature_importances_}, index=X_tree_train.columns).sort_values(['Importance'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_pred = xgb_model.predict(X_tree_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(df_validation['NET_LOSS'], xgb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ensemble the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_1= smf.ols(formula='NET_LOSS ~ xgb_pred + rf_pred + dt_pred', data=df_validation).fit()\n",
    "lm_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, refit the imputers and impute on `df_train` and `df_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_mean_final = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer_zero_final = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "categorical_imputer_final = CategoricalImputer(other_threshold=.01)\n",
    "imputer_missing_final = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_mean_final.fit(df_train[continuous_mean])\n",
    "df_train[continuous_mean] = imputer_mean_final.transform(df_train[continuous_mean])\n",
    "df_test[continuous_mean] = imputer_mean_final.transform(df_test[continuous_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_zero_final.fit(df_train[continuous_zero])\n",
    "df_train[continuous_zero] = imputer_zero_final.transform(df_train[continuous_zero])\n",
    "df_test[continuous_zero] = imputer_zero_final.transform(df_test[continuous_zero])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_imputer_final.fit(df_train[categorical_variables])\n",
    "df_train[categorical_variables] = categorical_imputer_final.transform(df_train[categorical_variables])\n",
    "df_test[categorical_variables] = categorical_imputer_final.transform(df_test[categorical_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_missing_final.fit(df_train[categorical_missing])\n",
    "df_train[categorical_missing] = imputer_missing_final.transform(df_train[categorical_missing])\n",
    "df_test[categorical_missing] = imputer_missing_final.transform(df_test[categorical_missing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, recreate the tree based models data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_linear_train_final, X_linear_train_final = dmatrices(formula_linear, df_train, return_type=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = build_design_matrices([X_linear_train_final.design_info], df_test, return_type=\"dataframe\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tree_train_final = df_train[continuous_features_trees + cat_ordinal_features_trees]\n",
    "y_tree_train_final = df_train['NET_LOSS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tree_train_final = df_train[continuous_features_trees + cat_ordinal_features_trees]\n",
    "y_tree_train_final = df_train['NET_LOSS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tree_train_patsy_final = dmatrix(formula_tree, df_train, return_type=\"dataframe\")\n",
    "X_tree_train_final = pd.concat([X_tree_train_final, X_tree_train_patsy_final], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder_final = OrdinalEncoder()\n",
    "ordinal_encoder_final.fit(X_tree_train_final[cat_ordinal_features_trees])\n",
    "X_tree_train_final[cat_ordinal_features_trees] = ordinal_encoder_final.transform(X_tree_train_final[cat_ordinal_features_trees])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tree_train_final.columns = X_tree_train_final.columns.str.replace('[', '(').str.replace(']', ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tree_test = df_test[continuous_features_trees + cat_ordinal_features_trees]\n",
    "y_tree_test = df_test['NET_LOSS']\n",
    "\n",
    "X_tree_test_patsy = build_design_matrices([X_tree_train_patsy_final.design_info], df_test, return_type=\"dataframe\")[0]\n",
    "\n",
    "X_tree_test = pd.concat([X_tree_test, X_tree_test_patsy], axis=1)\n",
    "\n",
    "X_tree_test[cat_ordinal_features_trees] = ordinal_encoder_final.transform(X_tree_test[cat_ordinal_features_trees])\n",
    "\n",
    "X_tree_test.columns = X_tree_test.columns.str.replace('[', '(').str.replace(']', ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model_final = DecisionTreeRegressor(max_depth=10,\n",
    "                                       min_samples_split=1000,\n",
    "                                       max_features=.5,\n",
    "                                       min_impurity_decrease=0,\n",
    "                                       random_state=201)\n",
    "dt_model_final.fit(X_tree_train_final, y_tree_train_final)\n",
    "dt_pred_final = dt_model_final.predict(X_tree_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_model_final = RandomForestRegressor(n_estimators=200,\n",
    "                                 max_features=20,\n",
    "                                 max_depth=20,\n",
    "                                 min_samples_split=100,\n",
    "                                 min_impurity_decrease=0,\n",
    "                                 random_state=201,\n",
    "                                 n_jobs=num_cpus)\n",
    "rf_model_final.fit(X_tree_train_final, y_tree_train_final)\n",
    "rf_pred_final = rf_model_final.predict(X_tree_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosted tree final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_final = XGBRegressor(max_depth=3,\n",
    "                         n_estimators = 90,\n",
    "                         learning_rate=.1,\n",
    "                         ccp_alpha=10,\n",
    "                         random_state=201)\n",
    "xgb_model_final.fit(X_tree_train_final, y_tree_train_final)\n",
    "xgb_pred_final = xgb_model_final.predict(X_tree_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = (1.1452*rf_pred_final + 0.0041*xgb_pred_final -0.0466*dt_pred_final)-65.4304"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, see you skill score on your final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(df_test['NET_LOSS'], final_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Write out the data with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_fresh['PREDICTIONS_NET_LOSS'] = final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_fresh.to_csv('FannieMaeTestWithPredictionsNetLoss.csv', sep='|')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "f7d1f0e70e3ca0134d66ef91f77ab860149e5029949fd8de746afe9338c863a1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
